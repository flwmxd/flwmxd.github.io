<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Advanced ECS Architecture</title>
    <link href="/2023/08/11/ECS/"/>
    <url>/2023/08/11/ECS/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>This article will try to explain the deep understanding of ecs architecture, which include the memory model, how to do in multi-thread with look-free and abstract interface for developer.</p><h1 id="CPU-Cache"><a href="#CPU-Cache" class="headerlink" title="CPU-Cache"></a>CPU-Cache</h1><p>Before talking about ecs architecture, I will try to explain an important knowledge “CPU-Cache”. </p><p><img src="/images/ecs/cpu-overview.png" alt="cpu-overview"></p><p>The main purpose of CPU cache is to reduce the latency or delay in data retrieval. When the CPU needs to access data, it first checks whether the data is available in the cache. If the data is present in the cache (cache hit), the CPU can retrieve it much faster than if it had to fetch the data from the main memory (cache miss). This helps to mitigate the performance bottleneck caused by the slower access times of main memory.</p><p>There are typically three levels of CPU cache: L1, L2, and L3. These levels form a hierarchy with different sizes and access speeds:</p><p>The cache hierarchy is designed to exploit the principle of <strong>spatial and temporal locality</strong>, where programs tend to access data and instructions that are close to what they’ve accessed recently. By keeping frequently accessed data in the cache, the CPU can operate more efficiently and reduce the time spent waiting for data from main memory.</p><h2 id="Spatial-and-Temporal-Locality"><a href="#Spatial-and-Temporal-Locality" class="headerlink" title="Spatial and Temporal Locality"></a>Spatial and Temporal Locality</h2><h3 id="Spatial-Locality"><a href="#Spatial-Locality" class="headerlink" title="Spatial Locality"></a>Spatial Locality</h3><p>Spatial locality refers to the tendency of a program to access data located near or close to the data that has been recently accessed. In other words, if a program accesses a particular memory location, it is likely to access nearby memory locations in the near future. This principle is rooted in the idea that data in memory is often stored in contiguous blocks, so accessing one piece of data often implies accessing neighboring data as well.<br>For example, when iterating over an array in a loop, spatial locality suggests that the next element to be accessed is likely to be stored in close proximity to the current element. Caches exploit spatial locality by fetching entire <strong>cache lines (blocks of contiguous memory)</strong> when a memory location is accessed. This way, if a program accesses one piece of data, it’s likely to access other nearby data soon, improving cache hit rates.</p><h3 id="Temporal-Locality"><a href="#Temporal-Locality" class="headerlink" title="Temporal Locality"></a>Temporal Locality</h3><p>Temporal locality refers to the tendency of a program to access the same data or instructions repeatedly over a short period of time. In other words, if a piece of data is accessed once, it’s likely to be accessed again in the near future. This principle is based on the observation that programs often exhibit repetitive behavior and reuse the same data multiple times within a short timeframe.<br>For instance, in a loop that processes the same data multiple times, temporal locality implies that the loop will repeatedly access the same data elements. Caches leverage temporal locality by keeping recently accessed data in the cache. If a piece of data is accessed once and stored in the cache, subsequent accesses to the same data can be served quickly from the cache.</p><h2 id="Who-is-Faster"><a href="#Who-is-Faster" class="headerlink" title="Who is Faster ?"></a>Who is Faster ?</h2><p>Assuming there is a matrix(N x M), we have these two approaches to iterate. which one is faster?</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i&lt;N; I++)<br>      <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j&lt;M; j++) <br>             <span class="hljs-built_in">print</span>(matrix[i][j]);<br></code></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i&lt;M; I++)<br>      <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j&lt;N; j++) <br>             <span class="hljs-built_in">print</span>(matrix[j][i]);<br></code></pre></td></tr></table></figure><h3 id="Iteration-By-Row"><a href="#Iteration-By-Row" class="headerlink" title="Iteration By Row"></a>Iteration By Row</h3><p>the answer is the <strong>first one</strong>. but why?  we use an example to explain. Look at the picture below. (ignored L3 cache.)</p><p>we use the first approch to iterate means iterating by row.</p><p>when the CPU wants to access matrix, it is not cache. so it would load a <strong>cache line</strong> into L2/L1.<br><img src="/images/ecs/load2.png" alt="cpu-load into L2"><br><img src="/images/ecs/load3.png" alt="cpu-load into L1"></p><p>now the cache line is mapped into L1/L2, the CPU can access matrix[N][0], matrix[N][1], matrix[N][2], etc quickly becuase they are in a contiguous block.</p><h3 id="Iteration-By-Column"><a href="#Iteration-By-Column" class="headerlink" title="Iteration By Column"></a>Iteration By Column</h3><p><img src="/images/ecs/load-by-column.png" alt="load-by-column"></p><p>but if we iterate matrix by column, in most cases the elements matrix[0][M] , matrix[1][M], and matrix[2][M] are not contiguous, which would cause a very high-frequency cache miss. Thus, the overall efficiency would be slower.</p><h2 id="ECS-Architecture-at-First-impression"><a href="#ECS-Architecture-at-First-impression" class="headerlink" title="ECS Architecture at First impression"></a>ECS Architecture at First impression</h2><p>The most obvious implementation of a component based model is the one that involves maps (or sort of) and objects taken directly from the typical OOP world</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs markdown">void system(std::vector<span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">GameObjects</span>&gt;</span></span> &amp;objects) &#123;<br><span class="hljs-code">    for(auto &amp;object: objects) &#123;</span><br><span class="hljs-code">        if(object.has&lt;position, velocity&gt;()) &#123;</span><br><span class="hljs-code">            auto &amp;pos = object.get&lt;position&gt;();</span><br><span class="hljs-code">            auto &amp;vel = object.get&lt;velocity&gt;();</span><br><span class="hljs-code">            //handle your logic.</span><br><span class="hljs-code">        &#125;</span><br><span class="hljs-code">    &#125;</span><br><span class="hljs-code">&#125;</span><br></code></pre></td></tr></table></figure><h3 id="Memory-Model"><a href="#Memory-Model" class="headerlink" title="Memory Model"></a>Memory Model</h3><p>In this example, so the memory looks like this. </p><pre><code class=" mermaid">stateDiagram-v2    position --&gt; object    velocity --&gt; object    components --&gt; object    position2 --&gt; object2    velocity2 --&gt; object2    components2 --&gt; object2</code></pre><h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems ?"></a>Problems ?</h3><ul><li>components are scattered around in memory and you’ve multiple jumps each and every time you access them.<ul><li>you don’t know at any time what are the game objects that match a given query and thus you must iterate all of them in each system</li><li>thus, there is a big performance issue.</li></ul></li></ul><h2 id="ECS-Architecture-at-Second-impression"><a href="#ECS-Architecture-at-Second-impression" class="headerlink" title="ECS Architecture at Second impression"></a>ECS Architecture at Second impression</h2><p>From the last example, we can see that <strong>the GameObject</strong> were nothing more than <strong>containers</strong> for components</p><ul><li>Components were stored in maps by game objects and every game object had its own set of components. <ul><li>Thus, It should be quite easy to get rid of these wrappers and change a bit the layout, so that components of a same type are stored together. it could look like this.</li></ul></li></ul><p>and then we can try to get rid of object and rename it to entity, becuase it is pointless. </p><pre><code class=" mermaid">graph LRentity1 --&gt; entity2 --&gt; entity3 --&gt; entity4 --&gt;  entityNcomponent --&gt; component2 --&gt; component3 --&gt; component4 --&gt; componentN </code></pre><p>but there is a problem, not all entities have same components. thus there are some solutions to solve it.</p><h3 id="Code-Appearance"><a href="#Code-Appearance" class="headerlink" title="Code Appearance"></a>Code Appearance</h3><p>ECS Architecture is totally different with OOP. so for each object, we can call it as <strong>entity</strong>, in this case it is only an integer for representing current object id and managed by central registry .<br>in order to accelerating accessing components data, each components would be storaged in a pools( continous memory block );</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Registry</span>&#123;<br><br>   <span class="hljs-function">entityId <span class="hljs-title">create</span><span class="hljs-params">()</span></span>&#123;....&#125;<br><br>   <span class="hljs-function"><span class="hljs-keyword">template</span>&lt;Comp&gt;</span><br><span class="hljs-function">   Comp&amp; <span class="hljs-title">emplace</span><span class="hljs-params">(Args&amp;&amp;...args)</span></span>&#123;....&#125;<br><br>   SparseSet&lt;entity&gt; entites;<br>&#125;;<br><br><span class="hljs-keyword">template</span>&lt;Comp&gt;<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">StoragePool</span><br>&#123;<br>   Comp[MAX_LENGHT] componts;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="Archetypes"><a href="#Archetypes" class="headerlink" title="Archetypes"></a>Archetypes</h3><p>The idea behind this approach can be summarized as follows: if an entity has a particular set of components, take the pool (also known as archetype) for the entities that have that same set (if it doesn’t already exist, create it) and assign the entity and all its components to that pool. Whenever you add/remove a component to/from an entity, pick up everything again and move the entity and all its components from a pool to the other, from an archetype to the other.</p><p><img src="/images/ecs/ArchetypeDiagram.png" alt="Archetypes"></p><ul><li>How does archetypes solve the problem of finding all the entities that have certain components?<ul><li><em><strong>The triditional way</strong></em> is iterating all the entities and test them to know if they have the desired components.</li><li><em><strong>The Archetype solution</strong></em> is iterating all the archetypes (much less than the entities), then return all the entities from the archetypes that are built for a set of components that contains at least the desired ones.</li></ul></li><li>What are the drawbacks ?<ul><li>Every time a component is added or removed, an entity and all its components are moved from an archetype to another one. This affects to an extent the construction and destruction of components</li><li>Main issue is the archetypes’s fragmentation. which means if you have a high number of possible combinations of components assigned to different entities at runtime and this will definitely affect the iterations to an extent by adding more and more jumps to find all the entities.</li></ul></li></ul><h3 id="Sparse-Sets"><a href="#Sparse-Sets" class="headerlink" title="Sparse Sets"></a>Sparse Sets</h3><p>It is different from Archetypes approach. Sparse sets is a clever data structure for storing sparse sets of integers on the range 0 .. u−1 and performing initialization, lookup, and insertion is time O(1) and iteration in O(n), where n is the number of elements in the set.</p><ul><li>in short, the sparse set would provides two array. one is <strong>dense[]</strong>, another one is <strong>sparse[]</strong>.</li></ul><p><img src="/images/ecs/sparse0b.png" alt="SparseSets"></p><p>the indirection isn’t required when you want to iterate all the values contained by the sparse set. It’s suffice to walk through the <strong>dense</strong> for that, from the first to the last.</p><p>how can we use the SpareSet to organize components? we can use the sparse set to create to pool and put components (look at the graph)</p><p><img src="/images/ecs/components.png" alt="organize components"></p><h5 id="Problems-1"><a href="#Problems-1" class="headerlink" title="Problems"></a>Problems</h5><p>but there still are some problems, if we only iterate one component, that must be faster. but if we want to iterate entities which contain component <strong>position</strong> and <strong>velocity</strong>. How do we do?</p><h3 id="View"><a href="#View" class="headerlink" title="View"></a>View</h3><p>The first implementation is <strong>view</strong>. which also provies <strong>single</strong> type views and <strong>multi-type</strong> views.</p><h4 id="Single-Views"><a href="#Single-Views" class="headerlink" title="Single Views"></a>Single Views</h4><p>Single type views are specialized to give a performance boost in all cases. (that’s because each components are in contious memory block and cache-friendly)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">auto</span> singleView = registry.<span class="hljs-built_in">view</span>&lt;position&gt;();<br></code></pre></td></tr></table></figure><p>There is nothing as fast as a single type view. They just walk through packed (actually paged) arrays of elements and return them directly.</p><pre><code class=" mermaid">graph LRentity1 --&gt; entity2 --&gt; entity3 --&gt; entity4 --&gt;  entityNposition --&gt; position2 --&gt; position3 --&gt; position4--&gt; position5</code></pre><h4 id="Multi-Type-Views"><a href="#Multi-Type-Views" class="headerlink" title="Multi Type Views"></a>Multi Type Views</h4><p>Multi type views iterate entities that have at least all the given components. </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">auto</span> muiltView = registry.<span class="hljs-built_in">view</span>&lt;position,velocity&gt;();<br></code></pre></td></tr></table></figure><pre><code class=" mermaid">graphsubgraph entityentity1 --&gt; entity2 --&gt; entity3 --&gt; entity4 --&gt;  entityNendsubgraph positon poolposition --&gt; position2 --&gt; position3 --&gt; position4--&gt; position5endsubgraph velocity poolvelocity1 --&gt; velocity2 --&gt; velocity4endentity1 --&gt; velocity1entity2 --&gt; velocity2entity4 --&gt; velocity4</code></pre><p>During construction, they look at the number of elements available in each pool and use the <strong>smallest set</strong> in order to speed up iterations.</p><pre><code class=" mermaid">graph LRsubgraph entityentity1 --&gt; entity2 --&gt; entity4endsubgraph velocity poolvelocity1 --&gt; velocity2 --&gt; velocity4end</code></pre><p>The performance is good when you <strong>only</strong> fetch velocity component(because it was the smallest set). but the performance could cause slower when you try to access other components becuase it would use indirect look-up.</p><pre><code class=" mermaid">graph LREntityId --&gt; ArrayIndexId --&gt; OtherComponent</code></pre><p>So is there any way to boost the performance when you want to iterate <strong>more components</strong>.</p><h3 id="Group"><a href="#Group" class="headerlink" title="Group"></a>Group</h3><p>Consider you want to iterate components <strong>position</strong> and <strong>velocity</strong>. Because of the way sparse sets work, you know components are all tightly packed in two arrays. Both of them contain some entities that have both the components and some others that have only one of the components. If you can arrange things so that all the entities that have both the components are at the top of the arrays while all the others are at the end, as a result the components will also be arranged accordingly. Iterations will benefit indecently from how things are laid out in this case, because all the entities that have both the components and the components themselves are tightly packed and sort in the same way at the beginning of their arrays. </p><p><img src="/images/ecs/group.png" alt="group"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Game Engine</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fake-Nanite</title>
    <link href="/2023/08/01/Learn-Nanite/"/>
    <url>/2023/08/01/Learn-Nanite/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Implement Nanite in my personal Game Engine( it is an experimental function now).<br>the goal is to render more than 10 billion faces.</p><h2 id="PPT-Click-Here"><a href="#PPT-Click-Here" class="headerlink" title="PPT Click Here"></a>PPT <a href="../../../../nodeppt/dist/nanite.html">Click Here</a></h2><p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/79aaFzgOso0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p><h1 id="GPUDriven-Pipeline"><a href="#GPUDriven-Pipeline" class="headerlink" title="GPUDriven Pipeline"></a>GPUDriven Pipeline</h1><p>Due to the increasingly high performance requirements of AAA games, the traditional rendering pipeline process cannot satisfy the real-time performance of the current AAA games. Therefore, in recent years, almost all engines will perform rendering-related operations on the GPU. one of the main purposes is to reduce the latencies between CPU-GPU, and at the same time release the CPU, so that the CPU can focus more on other tasks (AI, physical computing, network…)</p><h2 id="Triditional-Rendering"><a href="#Triditional-Rendering" class="headerlink" title="Triditional Rendering"></a>Triditional Rendering</h2><h2 id="Bottleneck-of-Triditional-Rendering"><a href="#Bottleneck-of-Triditional-Rendering" class="headerlink" title="Bottleneck of Triditional Rendering"></a>Bottleneck of Triditional Rendering</h2><ul><li>High CPU overload<ul><li>Frustum/Occlusion Culling</li><li>Prepare drawCall</li></ul></li><li>GPU idle time<ul><li>CPU can not follow up GPU</li></ul></li><li>High driver overhead<ul><li>GPU state exchange overhead when solving large amount of drawcalls</li></ul></li></ul><h2 id="GPUDriven-Rendering"><a href="#GPUDriven-Rendering" class="headerlink" title="GPUDriven Rendering"></a>GPUDriven Rendering</h2><ul><li>GPU decides what objects are actually drawed<ul><li>LOD Selection</li><li>Visiblity Culling on GPU</li></ul></li><li>No CPU/GPU roundtrip</li></ul><h2 id="What-Assassins-Creed-did"><a href="#What-Assassins-Creed-did" class="headerlink" title="What Assassins Creed did ?"></a>What Assassins Creed did ?</h2><ul><li><p>Use <strong>mesh cluster</strong> rendering</p></li><li><p>In some cases, the culling system is not efficiency. </p><ul><li>it could case overdraw, but parts of triangles players can not see it.</li></ul></li></ul><h1 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h1><ul><li>It is the minimum culling unit. <ul><li>each <strong>Cluster</strong> contains 128 triangles.</li></ul></li></ul><h1 id="Cluster-Group"><a href="#Cluster-Group" class="headerlink" title="Cluster Group"></a>Cluster Group</h1><ul><li>it is the simplification unit<ul><li>each <strong>Cluster Group</strong> contains 128 triangles.</li></ul></li></ul><h1 id="QEM-algorithm"><a href="#QEM-algorithm" class="headerlink" title="QEM algorithm"></a>QEM algorithm</h1><p>The central idea of the algorithm is to iteratively remove edges in the mesh through a process known as edge contraction which merges the vertices at an edge’s endpoints into a new vertex that optimally preserves the original shape of the mesh. This vertex position can be solved for analytically by minimizing the squared distance of each adjacent triangle’s plane to its new position after edge contraction. With this error metric, edges can be efficiently processed using a priority queue to remove edges with the lowest cost until the mesh has been sufficiently simplified.</p><h1 id="Culling"><a href="#Culling" class="headerlink" title="Culling"></a>Culling</h1><h2 id="Frustum-Culling"><a href="#Frustum-Culling" class="headerlink" title="Frustum Culling"></a>Frustum Culling</h2><h2 id="Occlusion-Culling"><a href="#Occlusion-Culling" class="headerlink" title="Occlusion Culling"></a>Occlusion Culling</h2><h1 id="Visibility-Buffer"><a href="#Visibility-Buffer" class="headerlink" title="Visibility Buffer"></a>Visibility Buffer</h1><h2 id="Overdraw"><a href="#Overdraw" class="headerlink" title="Overdraw"></a>Overdraw</h2><h2 id="Materil-ID"><a href="#Materil-ID" class="headerlink" title="Materil ID"></a>Materil ID</h2>]]></content>
    
    
    
    <tags>
      
      <tag>Geometric Processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SDFGI</title>
    <link href="/2023/01/15/SDFGI/"/>
    <url>/2023/01/15/SDFGI/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The project is a Hybrid GI solution, partly inspired by Lumen and DDGI. The main core idea is to provide a complete GI solution (including Indirect-Light, Infinite-Bounce, Emissive-Lighting, Glossy-Reflection, Shadow, AO) that can run on both Raytracing-supported and non-raytracing hardware. Due to the shortage of time, there are still some performance issues in this project, but as a throwaway, I believe it can provide some new ideas to most of the people who are interested in GI.</p><p>Before we start, let’s talk about DDGI (Dynamic-Diffuse-Global-Illumination)</p><p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/RfRbWnsdwx0" title="SDFGI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p><h2 id="DDGI"><a href="#DDGI" class="headerlink" title="DDGI"></a>DDGI</h2><p>Among the original implementations of DDGI ( <a href="https://morgan3d.github.io/articles/2019-04-01-ddgi/">description</a>, <a href="https://jcgt.org/published/0008/02/01/">paper</a>), DDGI is a real-time based probe and ray-tracing technique that mainly addresses the global light diffuse reflection term of dynamic scenes and light sources.</p><p>The traditional light probe is aim to collect light information from the perspective of the probe, and when a point on the screen is to going to be shaded, getting all probes that are close to the shading point, and then interpolation is performed. Therefore, the discrete information obtained by each probe can be reduced to continuous light information through interpolation, but once there is a sudden change in the signal, there can be light leakage problems.</p><p>However, the DDGI can avoid light leakage by saving the geometric information of the scene and then by probability.</p><h3 id="1-DDGI-Probe"><a href="#1-DDGI-Probe" class="headerlink" title="1.DDGI Probe"></a>1.DDGI Probe</h3><p>DDGI Probe uses an Octahedral Map method to store spherical information. The Octahedral Map is a good way to be compressed it into a Texture2D.<br>. (For those familiar with lumen, lumen also uses this way of expressing probes)</p><p><img src="/images/sdfgi/octahedral-map.png" alt="Oct"></p><h3 id="2-DDGI-Flow"><a href="#2-DDGI-Flow" class="headerlink" title="2.DDGI Flow"></a>2.DDGI Flow</h3><p>Step1. For each Probe generate 100-300 rays, and trace, to get the current ray’s hit the color and distance information. Therefore, it is necessary to create Texture2D to record the current information.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//Radiance for every ray.</span><br>internal.radiance-&gt;<span class="hljs-built_in">setName</span>(<span class="hljs-string">&quot;DDGI Raytrace Radiance&quot;</span>);<br>internal.radiance-&gt;<span class="hljs-built_in">buildTexture</span>(TextureFormat::R11G11B10F, pipeline.raysPerProbe, totalProbes);<br><br><span class="hljs-comment">//Direction and Depth for every ray.</span><br>internal.directionDepth-&gt;<span class="hljs-built_in">setName</span>(<span class="hljs-string">&quot;DDGI Raytrace Direction Depth&quot;</span>);<br>internal.directionDepth-&gt;<span class="hljs-built_in">buildTexture</span>(TextureFormat::RGBA16, pipeline.raysPerProbe, totalProbes);<br></code></pre></td></tr></table></figure><p>Since the diffuse GI of the point where the light hits is provided by the Probe of the previous frame, infinite bounce can be achieved in this way.</p><p><img src="/images/sdfgi/DDGI_Raytrace_Radiance.png" alt="Border"></p><p>The vertical axis is the ProbeId and the horizontal axis is the Radiance obtained for each ray.</p><p>Step2. Once the Radiance and Depth are obtained, the probe can be updated, i.e., the Radiance map is used to update the Irradiance of the Probe (while updating the distance information).</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span>(<span class="hljs-keyword">int32_t</span> i = <span class="hljs-number">0</span>;i&lt;<span class="hljs-number">2</span>;i++)<br>&#123;<br>    internal.depth[i]-&gt;<span class="hljs-built_in">setName</span>(<span class="hljs-string">&quot;DDGI Depth Probe Grid &quot;</span> + std::<span class="hljs-built_in">to_string</span>(i));<br>    internal.depth[i]-&gt;<span class="hljs-built_in">buildTexture</span>(TextureFormat::RG16F, depthWidth, depthHeight);    <span class="hljs-comment">//Depth and Depth^2</span><br><br>    internal.irradiance[i]-&gt;<span class="hljs-built_in">setName</span>(<span class="hljs-string">&quot;DDGI Irradiance Probe Grid &quot;</span> + std::<span class="hljs-built_in">to_string</span>(i));<br>    internal.irradiance[i]-&gt;<span class="hljs-built_in">buildTexture</span>(TextureFormat::RGBA16, irradianceWidth, irradianceHeight);<br>&#125;<br></code></pre></td></tr></table></figure><p>Step3. Border update</p><p>Because DDGI uses bilinear interpolation, special operations are required for the borders.</p><p><img src="/images/sdfgi/update-border.png" alt="Border"></p><p>Step4. Each pixel should be sampled by using 8 probes in its neighborhood.<br>One important is that when sampling with probes, we need to perform a visibility test to prevent light leakage problems.</p><ul><li>Visibility testing.<br>Since there is the problem of a wall between the probe and the sampling point, we need to perform visibility tests to avoid light leakage.</li></ul><p><img src="/images/sdfgi/Visibility-check.png" alt="Visibility"></p><p>In the above, we know that the distance information $d$ and the square of the distance $d^2$ is stored for each ray tracing, so we can use $d$ and $d^2$ for visibility tests</p><p>Here we introduce the Chebyshev inequality.</p><p>$P(r &gt; d) \leq \frac{Var}{Var + (d - mean) ^ 2}$</p><p>where $r$ is the distance that probe sees the nearest object in the direction of the shading point, $d$ is the distance between the shading point and probe, and $P(r &gt; d)$ represents the probability of no occlusion.</p><p>When $d &gt; mean$, we use Chebyshev weights, otherwise we use 1 directly</p><h3 id="Infinity-Bounce"><a href="#Infinity-Bounce" class="headerlink" title="Infinity Bounce"></a>Infinity Bounce</h3><p>The data from the previous probe frame is repeatedly used to perform an indirect illumination of the current scene, which is calculated to achieve an infinite bounces.</p><p><img src="/images/sdfgi/infinite-bounce.png" alt="bounce"></p><h2 id="SDF-Acceleration-Structure"><a href="#SDF-Acceleration-Structure" class="headerlink" title="SDF Acceleration Structure"></a>SDF Acceleration Structure</h2><p>SDF can be said to be the standard for the next generation engine, which not only provides ray tracing-like algorithms, but also supports SDF soft shadows, SDFAO and GPU-Particle collision detection.</p><p>During the Trace phase of DDGI’s Probe, we need to record the distance by each ray as well as the color information. The algorithm can be well implemented on hardware that supports ray tracing, but since most machines do not support hardware ray tracing, we need to use other ways to<br>get the color information.</p><p>First of all, SDF is a very good acceleration structure that can replace hardware ray tracing in some cases, so in current project, for machines that do not support hardware raytracing, we use SDF for replacement.</p><h2 id="SDF-Flow："><a href="#SDF-Flow：" class="headerlink" title="SDF Flow："></a>SDF Flow：</h2><h3 id="Generate-MeshDistanceField"><a href="#Generate-MeshDistanceField" class="headerlink" title="Generate MeshDistanceField"></a>Generate MeshDistanceField</h3><p>For each object in the scene generate its corresponding SDF, i.e. create Texture3D (R16F) with 32-128 (on-demand) resolutions to store distance information. And generate mipmap at the same time.</p><p>Since the SDF generation process uses ray tracing method, we can use Uniform Grid , KD-Tree , BVH-Tree to accelerate the construction. In this project, BHV-Tree is used for accelerated intersection.</p><p>A padding operation is also performed for each object to prevent the problem of leaks caused by objects that are too thin.</p><p><img src="/images/sdfgi/thin-mesh.png" alt="Visibility"></p><ul><li>Step2. Create GlobalDistanceField</li></ul><p>After having the above MeshDistanceField, GlobalDistanceField is created and its corresponding Mimmap is generated, since the current algorithm does not use the Lumen-like Detail-Trace (it may be implemented in a future version, but GlobalSDF can work better with DDGI-Cascade). Global is created to accelerate the SDF-Trace. </p><p><img src="/images/sdfgi/global-sdf.png" alt="GSDF"></p><h3 id="The-generation-algorithm-of-GlobalDistanceField"><a href="#The-generation-algorithm-of-GlobalDistanceField" class="headerlink" title="The generation algorithm of GlobalDistanceField"></a>The generation algorithm of GlobalDistanceField</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><br><span class="hljs-keyword">float</span> minDistance = DistanceMax;<br><br>minDistance *= <span class="hljs-built_in">imageLoad</span>(uGlobalSDF,voxelCoord).r;<br><br><span class="hljs-keyword">for</span> (uint i = <span class="hljs-number">0</span>; i &lt; pushConsts.objectsCount; i++)<br>&#123;<br>    uint objId = pushConsts.objects[i];<br>    MeshDistanceField meshDF = data[objId];<br>    minDistance = <span class="hljs-built_in">min</span>(minDistance, <span class="hljs-built_in">distanceToMeshSDF</span>(minDistance, meshDF, uMeshSDF[objId], voxelWorldPos));<br>&#125;<br></code></pre></td></tr></table></figure><p>In the game, we need to satisfy the real time, so we need to update the GlobalSDF when some elements in the scene are changed. Therefore, for dividing the space, i.e., updating the grid where dynamic objects are moved, in current algorithm we use the Uniform Grid (Chunk).</p><p><img src="/images/sdfgi/space-chunk.png" alt="GSDF"></p><p>A attention point is that the generation of Mimmap needs to be calculated manually, we can not calculate the mipmap value by average the original one, we have to choose the minmum value. the way looks like Hi-Z.</p><h3 id="SDF-Surface-Material"><a href="#SDF-Surface-Material" class="headerlink" title="SDF Surface Material"></a>SDF Surface Material</h3><p>Once we have the SDF acceleration structure, we can use the SDF for tracing, but the SDF is essentially a function describing the distance without any color or material information, so we need to build additional data types to generate the color and material information of the corresponding points of the SDF.</p><p>The implementation of Lumen<a href="https://docs.unrealengine.com/5.0/en-US/lumen-technical-details-in-unreal-engine/">Surface Cache</a> is a very efficient way to obtain SDF materials, and the same approach is also used in the current algorithm for SDF material representation</p><h4 id="Surface-GBuffer"><a href="#Surface-GBuffer" class="headerlink" title="Surface-GBuffer"></a>Surface-GBuffer</h4><p>Each object in the scene is captured on 6 sides and rendered to a different SurfaceAtlas. This process is the same as the traditional GBuffer process.<br>In the current algorithm, we generate Color/Normal/Depth/PBR/Emissive/LightCache Atlas and TileBuffer for each face.</p><p><img src="/images/sdfgi/chair.png" alt="Chair"></p><p><img src="/images/sdfgi/SurfaceCache.png" alt="SurfaceCache"></p><h4 id="Culling"><a href="#Culling" class="headerlink" title="Culling"></a>Culling</h4><p>In order to improve the overall rendering efficiency and avoid overdraw, we need to cull the objects in the scene (including the Tile associated with each object) in this stage.</p><h4 id="Light-Culling"><a href="#Light-Culling" class="headerlink" title="Light Culling"></a>Light Culling</h4><p>The lighting just light objects within its range for point and spot lights, and all potentially affected objects (as needed) for parallel light intensities.</p><h4 id="Object-Culling"><a href="#Object-Culling" class="headerlink" title="Object Culling"></a>Object Culling</h4><p>The objects outside the camera Far-Plane are culled, and the objects whose volume is smaller than the size threshold are ignored. Since the world space is divided into different Chunks, in order to ensure the performance of SDF after intersection, a culling operation is performed here, i.e., each Chunk keeps the objects that intersect with the current Chunk and sets a maximum limit on the number of objects.</p><h4 id="Obtain-Material"><a href="#Obtain-Material" class="headerlink" title="Obtain Material"></a>Obtain Material</h4><p>Obtaining the material for a point in the SDF space can be done as follows.</p><ol><li><p>Get the world coordinates after intersection, and project the current world coordinates to the corresponding ChunkId.</p></li><li><p>Iterate through all the Tile in the current Chunk, and judge the direction of the normal to the hit point and the orientation of the surface, if the current ray is almost parallel to the surface, then discard it. Otherwise, using normal to represent weight for its surface.</p></li><li><p>If not parallel to the current front, try to sample. Because there is a problem of accuracy after SDF intersection, we obtain 4 points around the current hit point for a bilinear interpolation to ensure the correctness of sampling when sampling.</p></li></ol><h3 id="Lighting"><a href="#Lighting" class="headerlink" title="Lighting"></a>Lighting</h3><h4 id="Direct-Lighting"><a href="#Direct-Lighting" class="headerlink" title="Direct Lighting"></a>Direct Lighting</h4><p>Afterwards, the Tile data of all affected objects are updated (SDF scene representation does not require real-time, so it can be operated in latency-frames), and a direct lighting is performed using the current SDF-GBuffer and stored in SurfaceLightCache. Because SDF provides visibility queries, the SurfaceDirectLight stage can be directly query shadows using SDF, as shown in the following figure.</p><p><img src="/images/sdfgi/SDFShadow.png" alt="SDFShadow-Surface"><br><img src="/images/sdfgi/SDFShadow-Surface.png" alt="SDFShadow-Surface"></p><h4 id="SDF-Infinity-Bounce"><a href="#SDF-Infinity-Bounce" class="headerlink" title="SDF Infinity Bounce"></a>SDF Infinity Bounce</h4><p>Infinite Bounce is the original function of DDGI. One point is that each indirect illumination result will be written back to SDFScene(LightCache) until convergence.</p><p><img src="/images/sdfgi/bounce.gif" alt="bounce2"></p><h2 id="Raytracing"><a href="#Raytracing" class="headerlink" title="Raytracing"></a>Raytracing</h2><p>In the case of hardware support, ray tracing can be enabled instead of SDF-Trace, and the other implementation algorithms do not differ much, so I won’t go into too much detail in this section.</p><h2 id="Reflection"><a href="#Reflection" class="headerlink" title="Reflection"></a>Reflection</h2><p>The reflection of the object surface is basically similar to the implementation of ray tracing and SDF-Trace, which mainly deals with Glossy Reflection and Specular Reflection<br>In the current algorithm implementation, when the surface roughness of the object is less than 0.05, we can think the surface as specular, which uses the specular illumination algorithm</p><p><img src="/images/sdfgi/ibl_specular_lobe.png" alt="specular"></p><p>When the material of the object surface is between 0.05 and 0.7, we consider the object material as Glossy at this time, because Glossy material produces reflected light in different directions. To ensure that the sampling is spatially homogeneous, we use <a href="https://belcour.github.io/blog/slides/2019-sampling-bluenoise/index.html">Blue noise and Sobol sequence</a> to generate samples, while using importance sampling to ensure its fast convergence (<a href="https://learnopengl.com/PBR/IBL/Specular-IBL">Reference</a>)</p><p>When the material roughness is greater than 0.7, we directly sample the current surface using a probe.</p><p>Notable points are: SDF can not be specular reflection (because the scene for SDF expression), in order to make up for the shortcomings of specular reflection, you can add screen space reflection in the SDF reflection (not implemented) <a href="https://docs.unrealengine.com/5.0/en-US/lumen-global-illumination-and-reflections-in-unreal-engine/">Lumen</a></p><p>SDF Reflection<br><img src="/images/sdfgi/sdf-glossy.png" alt="sdf-glossy"><br>Raytracing Reflection<br><img src="/images/sdfgi/raytrace-glossy.png" alt="raytrace-glossy"></p><h2 id="Soft-Shadow"><a href="#Soft-Shadow" class="headerlink" title="Soft Shadow"></a>Soft Shadow</h2><p>SDF can support soft shadow effect, but due to the accuracy of GlobalSDF, it is currently not recommended to use GlobalSDF for visibility calculation.</p><p>But provide a implementation. <a href="https://blog.demofox.org/2020/05/16/using-blue-noise-for-raytraced-soft-shadows/">soft shading based on ray tracing method</a> (higher cost)</p><p><img src="/images/sdfgi/shadow-compare.png" alt="shadow"></p><p>In the current algorithm, the number of samples for soft shadows 1, so there will be a lot of noise in the scene, in the project used the way of SVGF for denoising (the same reflection).</p><h2 id="Denoise"><a href="#Denoise" class="headerlink" title="Denoise"></a>Denoise</h2><p>Current algorithms for real-time ray tracing are basically based on primary sampling + denoise. In this algorithm, the Denoiser is mainly applied to Glossy reflections and Soft shadows.</p><p><img src="/images/sdfgi/reflection-denoise.png" alt="denoise"></p><h3 id="Gaussian-Filtering"><a href="#Gaussian-Filtering" class="headerlink" title="Gaussian Filtering"></a>Gaussian Filtering</h3><p>Gaussian filter is a kind of low-pass filter (its core is to find the average value in the filter kernel), the result after the filter is finished is the low frequency information, so the filtered result will appear the whole screen blurred. But for the game, low-pass filtering is not perfect. <a href="http://rastergrid.com/blog/2010/09/efficient-gaussian-blur-with-linear-sampling/">Reference</a></p><h3 id="Bilateral-Filtering"><a href="#Bilateral-Filtering" class="headerlink" title="Bilateral Filtering"></a>Bilateral Filtering</h3><p>Since Gaussian filtering is a low-pass filter, it does not maintain the boundaries well for images with large signal variations. Therefore, we can use bilateral filtering to process the image. In bilateral filtering, an important reference value is the change in brightness of the current pixel as a weight, i.e., a large change in brightness has a small weight. <a href="https://en.wikipedia.org/wiki/Bilateral_filter">Reference</a></p><p><img src="/images/sdfgi/filter.jpg" alt="Bilateral"></p><h3 id="Joint-Bilateral-Filtering"><a href="#Joint-Bilateral-Filtering" class="headerlink" title="Joint Bilateral Filtering"></a>Joint Bilateral Filtering</h3><p>In Gaussian filtering, we consider the distance between pixels as the contributing weight, and in bilateral filtering, we consider the brightness of the pixels. In real-time rendering we can use GBuffer to guide the filtering, and if more factors are considered (e.g. depth, normal features, etc.) we call it joint bilateral filtering.</p><h3 id="SVGF"><a href="#SVGF" class="headerlink" title="SVGF"></a>SVGF</h3><p>Spatiotemporal Variance-Guided Filtering<br><a href="https://research.nvidia.com/publication/2017-07_spatiotemporal-variance-guided-filtering-real-time-reconstruction-path-traced">NVIDA-Link</a> </p><p>Among the current GI algorithms we chose SVGF for denoise, which is a denoise algorithm running on Spatial and Temporal, using joint bilateral filtering with the addition of variance analysis and some other features.</p><p>In SVGF we use an <a href="https://jo.dreggn.org/home/2010_atrous.pdf">Atrous</a> for the filtering operation.</p><h4 id="Spatial-Filtering"><a href="#Spatial-Filtering" class="headerlink" title="Spatial Filtering"></a>Spatial Filtering</h4><p>In the spatial filtering part, SVGF uses the À-Trous wavelet filter, and also uses the progressive increase of Filter size to reduce the computational resource overhead, as shown in Fig.</p><p><img src="/images/sdfgi/SVGF-Spatial.jpg" alt="SVGF-Depth"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> yy = -RADIUS; yy &lt;= RADIUS; yy++)<br>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> xx = -RADIUS; xx &lt;= RADIUS; xx++)<br>    &#123;<br>        <span class="hljs-keyword">const</span> ivec2 coord = coord + <span class="hljs-built_in">ivec2</span>(xx, yy) * STEP_SIZE; <br>        <span class="hljs-comment">// Edge stop....</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h5 id="Depth-factor"><a href="#Depth-factor" class="headerlink" title="Depth factor"></a>Depth factor</h5><p><img src="/images/sdfgi/SVGF-Depth.png" alt="SVGF-Depth"></p><p>In Atrous, we first consider the depth factor, as in Figure A/B, because AB is on the same face, so in the process of filtering, AB should contribute to each other, but there is a difference in the depth of AB, so for the processing of depth, we will be more biased to use the gradient of depth for the operation.</p><h5 id="Normal-factor"><a href="#Normal-factor" class="headerlink" title="Normal factor"></a>Normal factor</h5><p>Next we consider the effect of normals on denoise</p><p><img src="/images/sdfgi/SVGF-Normal.png" alt="SVGF-Normal"></p><p>As shown in the figure, if the angle between the two points of AB is smaller, it means that the direction of the two points may exist all the way, so the value of $ W_n $ is more convergent with 1</p><h5 id="Color-factor"><a href="#Color-factor" class="headerlink" title="Color factor"></a>Color factor</h5><p><img src="/images/sdfgi/SVGF-Color.png" alt="SVGF-Color"></p><p>The contribution of color to the weight is mainly reflected in the change of color brightness, i.e. Luminance, but due to the existence of noise, the original noised color may be obtained, so we can correct this problem by variance.</p><h4 id="Temporal-Filtering"><a href="#Temporal-Filtering" class="headerlink" title="Temporal Filtering"></a>Temporal Filtering</h4><p>The filtering in time mainly reuses the information from the previous frame, in this case we can think of increasing the number of samples of the current pixel.</p><p>In order to obtain information about the previous frame’s pixels in a reasonable way, we use the Motion Vector method to record the direction of the camera’s movement, so that in the current frame, the specific pixels of the previous frame can be mapped back through the Motion Vector.</p><h5 id="Normal-factor-1"><a href="#Normal-factor-1" class="headerlink" title="Normal factor"></a>Normal factor</h5><p>The main check is when the angle between two points is less than a specific threshold value, and the normal can be judged using the following formula</p><p>$Fn = |dot(n(a),n(b))| - T $(T is the threshold value)</p><p>If the current Fn is greater than 0, then the two points are not in the same plane and the current point is discarded.</p><h5 id="MeshID"><a href="#MeshID" class="headerlink" title="MeshID"></a>MeshID</h5><p>Because we can use the information of GBuffer in this stage, we can get the MeshID information of the corresponding point, and discard the point if the MeshIDs of two points of A/B do not match.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Graphics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Maple Engine</title>
    <link href="/2021/09/10/MapleEngine/"/>
    <url>/2021/09/10/MapleEngine/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Maple-Engine is my own engine for leaning mordern engine and rendering technique. Because it is early stage now , I did not open source it.</p><p><img src="/images/MapleEngine.png"></p><h3 id="Realtime-Volumetric-Cloud"><a href="#Realtime-Volumetric-Cloud" class="headerlink" title="Realtime Volumetric Cloud"></a>Realtime Volumetric Cloud</h3><p><img src="/images/cloud.png"></p><h3 id="Atmospheric-Scattering"><a href="#Atmospheric-Scattering" class="headerlink" title="Atmospheric Scattering"></a>Atmospheric Scattering</h3><p><img src="/images/Atmosphere.png"></p><h3 id="Light-Propagation-Volumes"><a href="#Light-Propagation-Volumes" class="headerlink" title="Light Propagation Volumes"></a>Light Propagation Volumes</h3><p><img src="/images/LPV.png"></p><h2 id="Current-Features"><a href="#Current-Features" class="headerlink" title="Current Features"></a>Current Features</h2><ul><li>OpenGL/Vulkan backends </li><li>Entity-component system( Based on entt )</li><li>PBR/IBL</li><li>Directional lights + Cascaded shadow maps</li><li>Soft shadows (PCF)</li><li>Screen Space Ambient Occlusion (SSAO)</li><li>Screen Space Reflections(SSR)</li><li>Lua Scripting</li><li>C# Scripting (Mono)</li><li>Ray-marched volumetric lighting</li><li>Atmospheric Scattering</li><li>Realtime Volumetric Cloud ( Based on Horizon Zero Dawn )</li><li>Post-process effects (Tone-Mapping.)</li><li>Event system</li><li>Input (Keyboard, Mouse)</li><li>Profiling (CPU &amp; GPU)</li><li>Batch2D Renderer</li></ul><h2 id="Roadmap"><a href="#Roadmap" class="headerlink" title="Roadmap"></a>Roadmap</h2><table><thead><tr><th align="left">Feature</th><th align="left">Completion</th><th align="left">Notes</th></tr></thead><tbody><tr><td align="left">Reflective Shadow Map</td><td align="left">50%</td><td align="left">High priority</td></tr><tr><td align="left">Screen space global illumination</td><td align="left">0%</td><td align="left">High priority</td></tr><tr><td align="left">Light propagation volumes</td><td align="left">0%</td><td align="left">High priority</td></tr><tr><td align="left">C# scripting</td><td align="left">80%</td><td align="left">Using Mono (no engine API exposed yet)</td></tr><tr><td align="left">Animation and State Machine</td><td align="left">0%</td><td align="left">Implemented in Raven Engine</td></tr><tr><td align="left">Vulkan porting</td><td align="left">90%</td><td align="left">support Compute and Tessellation shader</td></tr><tr><td align="left">Precomputed Atmospheric Scattering</td><td align="left">-</td><td align="left">Low priority</td></tr><tr><td align="left">Subsurface Scattering</td><td align="left">-</td><td align="left">Low priority</td></tr><tr><td align="left">Ray traced shadows</td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left">Ray traced reflections</td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left">Linux support</td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left">Android support</td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left">Mac support</td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left">iOS support</td><td align="left">-</td><td align="left">-</td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>Game Engine</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Loop Subdivision</title>
    <link href="/2021/04/10/LoopSubdivision/"/>
    <url>/2021/04/10/LoopSubdivision/</url>
    
    <content type="html"><![CDATA[<h1 id="SubDivision"><a href="#SubDivision" class="headerlink" title="SubDivision"></a>SubDivision</h1><p>Subdivision is a powerful and easily implemented algorithm used, in its simplest application, to smooth meshes. The underlying concepts are derived from spline refinement algorithms, but the idea is that there exists a well-defined smooth surface associated with any given input mesh (the exact surface depends on the subdivision algorithm used.) We define a refinement operation, that takes the input mesh and generates an output mesh that is closer to the surface. If we apply this refinement process infinitely, we will exactly achieve the target surface. However, after just a few levels of refinement, we are generally close enough to the limit surface that they are visually indistinguishable. Depending on the type of input mesh (triangular, quadrilateral, etc.) we must use a different subdivision algorithm, and within triangular meshes there are still a wide array, although some are visually superior to others. Quadrilateral based meshes generally use Catmull-Clark, while triangular based meshes generally use loop subdivision.</p><h1 id="Loop-Subdivision"><a href="#Loop-Subdivision" class="headerlink" title="Loop Subdivision"></a>Loop Subdivision</h1><blockquote><p>Loop subdivision surface is an approximating subdivision scheme developed by Charles Loop in 1987 for triangular meshes.<br><a href="https://en.wikipedia.org/wiki/Loop_subdivision_surface">wikipedia</a></p></blockquote><p>All subdivision algorithms start by replacing the geometric element (in our case, a triangle) with smaller versions of the same element. In the 3D case of tetrahedrons, this is very complicated, but in the case of triangles it is trivial:</p><p><img src="/images/loop/1.png"></p><p>For every edge in the source mesh, add a vertex (shown in blue,) and for every triangle on the mesh, create the four triangles shown above. The exact geometric location of these new edge vertices, and the new coordinates of the intial vertices, are all determined by the subdivision scheme. In every case, however, they are linear combinations of the source mesh vertices, and the source mesh vertices that contribute to a point’s new location are only a few edge or triangle hops away. The edge vertices are the easiest to deal with:</p><p><img src="/images/loop/2.png"></p><p>Every edge in the source mesh has two adjacent faces (we’ll deal with boundaries later,) and from these we can find the two “opposite” vertices. Of course each edge also has two connected vertices, and we just take the a linear combination of the source vertices in the ratios shown above, and we have the location of the vertex associated with this edge.</p><p><img src="/images/loop/3.png"></p><p>Every vertex in the source mesh is also in the subdivided mesh. Its new position is computed above, and depends on all the vertices connected to the vertex by an edge. The number of such vertices, n, determines the constant beta. There are many options avaliable, but the simplest choice is</p><p><img src="/images/loop/4.png"></p><p>The boundary cases are based off basic spline refinement schemes and are equally simple. For a new edge vertex:</p><p><img src="/images/loop/5.png"></p><p>And for a boundary vertex:</p><p><img src="/images/loop/6.png"></p><p><img src="/images/loop-subdivision.gif"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Geometric Processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Free-Form Deformation</title>
    <link href="/2021/04/10/FFD/"/>
    <url>/2021/04/10/FFD/</url>
    
    <content type="html"><![CDATA[<h2 id="Repo-is-here"><a href="#Repo-is-here" class="headerlink" title="Repo is here"></a>Repo is <a href="https://github.com/flwmxd/Simulations">here</a></h2><p>This is an OpenGL Simulation Application, which written by c++.</p><p>This is a assignments 1 from course Animation Simulation University of Leeds</p><ul><li><ol><li>A GUI to load, display, and write-out 2D triangular meshes. You can define your own file format. (6g) </li></ol></li><li><ol start="2"><li>A 2D free-form deformation tool using a grid-based cage. The user should be able to use the mouse to move any vertex of the cage and deform a given 2D triangular mesh. The weights of a mesh vertex with respect to its associated cage vertices should be computed by bilinear interpolation. (6g) </li></ol></li><li><ol start="3"><li>A 2D free-form deformation tool using a triangular-mesh cage. The user should be able to move the mouse to position any vertex of the cage and deform a given 2D triangular mesh. The weights of a mesh vertex with respect to its associated cage vertices should be computed by Barycentric coordinates. (4g) </li></ol></li><li><ol start="4"><li>A systematic (grid-based or triangular) cage deformation. Different from 2 and 3, when the user moves one cage vertex, other cage vertices should move according to some arbitrarily defined functions, e.g. influence attenuation based on distance like what we taught in class. The given mesh will then be deformed accordingly. (4g) </li></ol></li><li><ol start="5"><li>A 3D free-form deformation tool using a grid-based cage to deform a 3D mesh, similar to 2 in 2D, except that the weights of a mesh vertex with respect to its associated cage vertices should be computed by trilinear interpolation. (5g) </li></ol></li></ul><h1 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h1><h3 id="Barycentric-coordinates"><a href="#Barycentric-coordinates" class="headerlink" title="Barycentric coordinates"></a>Barycentric coordinates</h3><p>For $ P, t_1 + t_2 + t_3 = 1 $, and they are proportional to $△A_2A_3P, △A_1A_3P and △A_1A_2P $ All t &gt; 0</p><p><img src="/images/ffd/Barycentric.png" alt="Barycentric"></p><h3 id="Bilinear-Interpolation"><a href="#Bilinear-Interpolation" class="headerlink" title="Bilinear Interpolation"></a>Bilinear Interpolation</h3><p>P and point out some of its properties:</p><p>$$<br>    P = \frac{(x_2 - x)(y_2 - y)}{(x_2 - x_1)(y_2 - y_1)} Q_{11} + \frac{(x - x_1)(y_2 - y)}{(x_2 - x_1)(y_2 - y_1)} Q_{21}<br>    + \frac{(x_2 - x)(y - y_1)}{(x_2 - x_1)(y_2 - y_1)} Q_{12} + \frac{(x - x_1)(y - y_1)}{(x_2 - x_1)(y_2 - y_1)} Q_{22}<br>$$</p><p><img src="/images/ffd/bilinear.png" alt="bilinear"></p><p>In the ffd-2d, current selected point we called it as <strong>Control Point</strong>, if the cage is rectangle, using Bilinear Interpolation to calculate postion around it, otherwise it is triangle, using Barycentric coordinates to calculate</p><h3 id="FFD-2D"><a href="#FFD-2D" class="headerlink" title="FFD-2D"></a>FFD-2D</h3><p><img src="/images/ffd2d.gif" alt="ffd2d"></p><p>In the ffd-3d, using <a href="https://en.wikipedia.org/wiki/Bernstein_polynomial">Bernstein polynomial</a> to implement</p><h3 id="FFD-3D"><a href="#FFD-3D" class="headerlink" title="FFD-3D"></a>FFD-3D</h3><p><img src="/images/ffd3d.gif" alt="FFD3D"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Simulation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cloth Simulation</title>
    <link href="/2021/04/10/Simulation-Cloth-Wind/"/>
    <url>/2021/04/10/Simulation-Cloth-Wind/</url>
    
    <content type="html"><![CDATA[<p>This is an OpenGL Simulation Application, which written by c++.</p><p>They are assignments 3 from course Animation Simulation University of Leeds</p><h2 id="Repo-is-here"><a href="#Repo-is-here" class="headerlink" title="Repo is here"></a>Repo is <a href="https://github.com/flwmxd/Simulations">here</a></h2><p>You will build a cloth simulation system to show different fabric behaviours. It should come with a GUI where you can load/write cloth mesh files (OBJ<br>files). It should also be able to detect and resolve simple cloth-object collisions. </p><p>For your hard work, the Mistress Fingercutter has graciously agreed to give you rewards for: </p><ol><li><p>A GUI to load OBJ files and render them in OpenGL (simple rendering without textures is<br>acceptable). (2g) </p></li><li><p>The same GUI to write OBJ files. This should write a single frame into an OBJ file.(2g) </p></li><li><p>A mass-spring model to simulate a piece of cloth with arbitrary shapes specified by an OBJ file.<br>The vertices are modelled as point masses and the edges are modelled as springs. The<br>simulation is free falling onto the floor. (4g) </p></li><li><p>Simulation Scenario 1 (SS1): a square piece of cloth floating horizontally in the air at first, which<br>then free-falls onto a sphere on the ground. (4g) </p></li><li><p>Simulation Scenario 2 (SS2): a square piece of cloth floating horizontally in the air first, then<br>free-falling with two adjacent corners fixed in the space. (4g) </p></li><li><p>SS1 with the sphere rotating in-place round the Y axis (up-axis) and friction between the sphere<br>and the cloth. (3g) </p></li><li><p>SS2 with wind blowing. (3g) </p></li><li><p>Texture, lighting, and the ability to export the simulation result to a video. (3g) </p></li></ol><p>There are several places in town where you can find assistance.<br>Hint 1: The Mesh Master at the Asylum of Geometry. (You can find the specs of OBJ files at<br><a href="https://en.wikipedia.org/wiki/Wavefront_.obj_file">https://en.wikipedia.org/wiki/Wavefront_.obj_file</a> and some sample code in Minerva under Lab<br>Resources) </p><p>Hint 2: The Geomonger (a person who sells raw geometries) at the Mesh take-out place. (You can find other mesh software to help you debug and test your system. Meshlab: <a href="http://www.meshlab.net/">http://www.meshlab.net/</a>) </p><p>Hint 3: It is acceptable to export frames into images first, then use tools such as ffmpeg to make videos. </p><p>Hint 4: The collision between the ball and the cloth should not be sticky. It can be implemented by a post-processing step to move any vertex inside the sphere to the closest point on the surface of the sphere. </p><p>Hint 5: Wind blowing can be modelled as a steady force in a fixed direction. Hint 6: At each cloth vertex, the friction force between the sphere and the cloth is always in the opposite direction of their relative motions. The plane the friction force lies in is orthogonal to the surface normal of the sphere at that cloth vertex</p><h1 id="1-Spring-Mass-System"><a href="#1-Spring-Mass-System" class="headerlink" title="1. Spring-Mass System"></a>1. Spring-Mass System</h1><p>we consider the cloth as a collection of particles interconnected with three types of springs:</p><p><img src="/images/mass-spring/1.jpg" alt="springs"></p><h2 id="1-1-Particles"><a href="#1-1-Particles" class="headerlink" title="1.1 Particles"></a>1.1 Particles</h2><p>We assume the cloth contains n×n (evenly spaced) particles and use [<em><strong>i</strong></em>,<em><strong>j</strong></em>] (where 0≤<em>i</em>,<em>j</em>&lt;n) to denote the particle located at the i-th row and j-th column. Each particle  [<em><strong>i</strong></em>,<em><strong>j</strong></em>] has the following states:</p><ul><li><strong>Mass</strong> <em>m</em> (we assume all particles to have identical masses).</li><li><strong>Position</strong> $x_{i,j}(t)$.</li><li><strong>Velocity</strong> $v_{i,j}(t)$ which equals the derivate of $x_{i,j}(t)$ with respect to t: $v_{i,j}(t)$ = $\dot x_{i,j}(t)$.</li></ul><h2 id="1-2-Springs"><a href="#1-2-Springs" class="headerlink" title="1.2 Springs"></a>1.2 Springs</h2><p>There are three types of springs connecting all particles: structural, shear, and flexion (bend).</p><ul><li><strong>Structural</strong>: each particle [<em><strong>i</strong></em>,<em><strong>j</strong></em>] is connected to (up to) four particles via structural connections: [<em><strong>i</strong></em>,<em><strong>j+1</strong></em>], [<em><strong>i</strong></em>,<em><strong>j−1</strong></em>], [<em><strong>i+1</strong></em>,<em><strong>j</strong></em>], [<em><strong>i−1</strong></em>,<em><strong>j</strong></em>].</li><li><strong>Shear</strong>: each particle [<em><strong>i</strong></em>,<em><strong>j</strong></em>] is connected to (up to) four particles via shear connections: [<em><strong>i+1</strong></em>,<em><strong>j+1</strong></em>], [<em><strong>i+1</strong></em>,<em><strong>j−1</strong></em>], [<em><strong>i−1</strong></em>,<em><strong>j−1</strong></em>], [<em><strong>i−1</strong></em>,<em><strong>j+1</strong></em>].</li><li><strong>Flexion</strong>: each particle [<em><strong>i</strong></em>,<em><strong>j</strong></em>] is connected to (up to) four particles via flexion connections: [<em><strong>i</strong></em>,<em><strong>j+2</strong></em>], [<em><strong>i</strong></em>,<em><strong>j−2</strong></em>], [<em><strong>i+2</strong></em>,<em><strong>j</strong></em>], [<em><strong>i−2</strong></em> , <em><strong>j</strong></em>].<br>Thus, each particle can have up to 12 directly connected neighbors.</li></ul><p><strong>Stiffness</strong>: in this project, we assume all structural springs to have stiffness K0, shear springs K1, and flexion strings K2.</p><p><strong>Rest length</strong>: generally, a piece of cloth is considered to be in its rest state when there is no deformation (caused by stretching, shearing, or bending).</p><h2 id="1-3-Forces"><a href="#1-3-Forces" class="headerlink" title="1.3 Forces"></a>1.3 Forces</h2><p>There are three types of forces you will need to consider for this project:</p><ul><li><p><strong>Spring forces</strong>: given spring connecting two particles located at p and q with stiffness K and rest length L0, the spring force acting on p equals<br>$F_{spring}=K(L_0−∥p−q∥)\frac{p−q}{∥p−q∥}$.<br>This force changes over time as the particles move.</p></li><li><p><strong>Gravity</strong>: each particle is affected by (simple) gravity given by</p></li></ul><p>$$F_G=\left( \begin{array} c 0 \ -mg \ 0 \end{array} \right) $$</p><ul><li><strong>Damping</strong>: for a particle with velocity v, the amount of damping force it receives equals $F_{damp} =−c_d * v$ where $c_d$&gt;0 is a constant. This force changes over time as $v$ changes.</li></ul><h2 id="1-4-Integration"><a href="#1-4-Integration" class="headerlink" title="1.4 Integration"></a>1.4 Integration</h2><p>Animating a mass-spring system involves solving an initial value problem. In particular, for each particle [i,j] you are given its mass m (which stays constant for all particles), initial position $x_{i,j}(0)$ and velocity $v_{i,j}(0)$. Given $Δt$&gt;0, you need to use <strong>Euler’s method</strong> to compute the position and velocity of each particle at time-steps $Δt, 2Δt, 3Δt$, ….</p><p>You need to pin two particles [ <em>n−1</em> ,<em>0</em>]  and [ <em>n−1</em>,<em>n−1</em>], which correspond to the upper left and right corners of the cloth. That is, the positions of these particles are fixed to their inital values. If no particle is pinned in place, the entire cloth will undergo a never-ending free fall because of gravity!</p><p><img src="/images/cloth-.gif" alt="cloth"></p><h2 id="1-5-Wind"><a href="#1-5-Wind" class="headerlink" title="1.5 Wind"></a>1.5 Wind</h2><p>Applying a wind force to the system involves applying a wind force a direction or directions in every particles.</p><p>$$ F =F_G + F_{wind} $$</p><h3 id="Cloth-Wind"><a href="#Cloth-Wind" class="headerlink" title="Cloth Wind"></a>Cloth Wind</h3><p><img src="/images/cloth-wind.gif" alt="wind"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Simulation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Simplification Using Quadric Error Metrics</title>
    <link href="/2021/04/10/Simplification/"/>
    <url>/2021/04/10/Simplification/</url>
    
    <content type="html"><![CDATA[<p>Repo is <a href="https://github.com/flwmxd/Simplification">Here</a></p><p>This project is my Geometric Processing Assignment in University of Leeds, which implements two parts</p><p>Tarjan’s strongly connected components<br>Surface Simplification Using Quadric Error Metrics. <a href="https://dl.acm.org/doi/10.1145/258734.258849">Paper</a></p><h1 id="Simplification-Mesh-with-90-vertices"><a href="#Simplification-Mesh-with-90-vertices" class="headerlink" title="Simplification Mesh with 90% vertices"></a>Simplification Mesh with 90% vertices</h1><p><img src="/images/simplification/90%25.png" alt="Simplification-90%"></p><h1 id="Simplification-Mesh-with-10-vertices"><a href="#Simplification-Mesh-with-10-vertices" class="headerlink" title="Simplification Mesh with 10% vertices"></a>Simplification Mesh with 10% vertices</h1><p><img src="/images/simplification/10%25.png" alt="Simplification-10%"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Geometric Processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fluid Simulation</title>
    <link href="/2021/04/10/Simulation-Fluid/"/>
    <url>/2021/04/10/Simulation-Fluid/</url>
    
    <content type="html"><![CDATA[<p>This is an OpenGL Simulation Application, which written by c++.</p><p>They are assignments 4 from course Animation Simulation University of Leeds</p><h2 id="Repo-is-here"><a href="#Repo-is-here" class="headerlink" title="Repo is here"></a>Repo is <a href="https://github.com/flwmxd/Simulations">here</a></h2><h1 id="Project-Descripton"><a href="#Project-Descripton" class="headerlink" title="Project Descripton"></a>Project Descripton</h1><p>You will build a fluid simulator based on Smoothed<br>Particle Hydrodynamics (SPH). It should come with a GUI where you can specify the initial state of the water and the environment. It should be able to enable/disable different forces. </p><p>For your hard work, the Lady Partypooper will reward you for: </p><ol><li>A GUI to set up the simulation environment, run the simulation and render the results. (5g) </li><li>A 2D SPH simulation with a water tank and a water blob floating in the air at first then freefalling under gravity (see Hint 1). The only forces considered here are gravity and internal<br>pressure. The simulation should also consider the collisions between the particles and the water<br>tank (but not between particles). The integration scheme should be the Leapfrog scheme. The<br>kernel should be the poly6 kernel we introduced in the class. (8g) </li><li>The same simulation as 2, but replacing the kernel with Debrun’s spiky kernel for pressure<br>computation (only for pressure). (4g) </li><li>The same simulation as 3, plus the viscosity and the viscosity kernel introduced in the class. (4g) </li><li>The same simulation as 4, plus the surface tension with the poly6 kernel. (4g)<br>The total reward is 5g + 8g + 4g + 4g + 4g = 25g. </li></ol><p>Hint 1: the initial state of the simulation:<br><img src="/images/fluid/1.png" alt="fluid"><br>Hint 2: the rendering can be performed via one of the following approaches: </p><ul><li>drawing each particle as a small disc, or </li><li>using textures to fill each grid cell with transparencies controlled by the local particle density, or </li><li>marching cubes to draw the free surfaces<br>Hint 3: the details are in the slides as well as <strong><a href="https://matthias-research.github.io/pages/publications/sca03.pdf">https://matthias-research.github.io/pages/publications/sca03.pdf</a></strong></li></ul><h1 id="Particle-Hydrodynamics"><a href="#Particle-Hydrodynamics" class="headerlink" title="Particle Hydrodynamics"></a>Particle Hydrodynamics</h1><p>Smoothed-particle hydrodynamics (SPH) is a computational method used for simulating fluid flows. It was developed by Gingold and Monaghan (1977) and Lucy (1977) initially for astrophysical problems. It has been used in many fields of research, including astrophysics, ballistics, volcanology, and oceanography. It is a mesh-free Lagrangian method (where the coordinates move with the fluid), and the resolution of the method can easily be adjusted with respect to variables such as the density.</p><p><img src="/images/fluid.gif" alt="ffd2d"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Simulation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Skeleton Animation and Inverse Kinematics</title>
    <link href="/2021/04/10/Simulation-IK/"/>
    <url>/2021/04/10/Simulation-IK/</url>
    
    <content type="html"><![CDATA[<p>This is an OpenGL Simulation Application, which written by c++.</p><p>They are assignments 2 from course Animation Simulation University of Leeds</p><h2 id="Repo-is-here"><a href="#Repo-is-here" class="headerlink" title="Repo is here"></a>Repo is <a href="https://github.com/flwmxd/Simulations">here</a></h2><h2 id="Simulation-2-is-Inverse-Kinematics"><a href="#Simulation-2-is-Inverse-Kinematics" class="headerlink" title="Simulation 2 is Inverse Kinematics"></a>Simulation 2 is Inverse Kinematics</h2><p>In computer animation and robotics, inverse kinematics is the mathematical process of calculating the variable joint parameters needed to place the end of a kinematic chain, such as a robot manipulator or animation character’s skeleton, in a given position and orientation relative to the start of the chain.</p><h3 id="Numerical-solutions-to-IK-problems"><a href="#Numerical-solutions-to-IK-problems" class="headerlink" title="Numerical solutions to IK problems"></a>Numerical solutions to IK problems</h3><h4 id="The-Jacobian-inverse-technique"><a href="#The-Jacobian-inverse-technique" class="headerlink" title="The Jacobian inverse technique"></a>The Jacobian inverse technique</h4><p><a href="https://en.wikipedia.org/wiki/Inverse_kinematics">References in wiki</a></p><p><img src="/images/IK.gif" alt="ffd2d"></p><h3 id="Skeleton-Animation"><a href="#Skeleton-Animation" class="headerlink" title="Skeleton Animation"></a>Skeleton Animation</h3><p>Skeletal animation or rigging is a technique in computer animation in which a character (or other articulated object) is represented in two parts: a surface representation used to draw the character (called the mesh or skin) and a hierarchical set of interconnected parts (called bones, and collectively forming the skeleton or rig), a virtual armature used to animate (pose and keyframe) the mesh. </p><p>In this project, the method is implementing the BVH files animation.</p><p><img src="/images/skeleton-animation.gif" alt="skeleton-animation"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Simulation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OpenGL-SoftRenderer</title>
    <link href="/2021/04/10/OpenGL-SoftRenderer/"/>
    <url>/2021/04/10/OpenGL-SoftRenderer/</url>
    
    <content type="html"><![CDATA[<p>This is an OpenGL software-renderer, which written by c++.</p><h1 id="Main-Features"><a href="#Main-Features" class="headerlink" title="Main Features"></a>Main Features</h1><h3 id="1-Draw-lines-by-bresenham"><a href="#1-Draw-lines-by-bresenham" class="headerlink" title="1. Draw lines by bresenham"></a>1. Draw lines by bresenham</h3><h3 id="2-Depth-test"><a href="#2-Depth-test" class="headerlink" title="2. Depth test"></a>2. Depth test</h3><h3 id="3-Transforms"><a href="#3-Transforms" class="headerlink" title="3. Transforms"></a>3. Transforms</h3><h3 id="4-Textures"><a href="#4-Textures" class="headerlink" title="4. Textures"></a>4. Textures</h3><h3 id="5-Phong-Shading"><a href="#5-Phong-Shading" class="headerlink" title="5. Phong Shading"></a>5. Phong Shading</h3><p><img src="/images/Texture.png" alt="OpenGL"></p><p><img src="/images/Color.png" alt="Fake OpenGL"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Graphics</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
